import json
import os
import glob
import numpy as np
from argparse import ArgumentParser
from scipy.special import comb # ğŸ’¡ í‘œì¤€ ì¡°í•©(comb) ê³„ì‚°ì„ ìœ„í•´ ì¶”ê°€

# util.pyì— initialize_result_dictê°€ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ë§Œì•½ ì—†ë‹¤ë©´ ì´ ë¼ì¸ì€ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤.
# from util import initialize_result_dict

CURRENT_DIR_PATH = os.path.abspath(os.path.dirname(__file__))


def get_parser():
    parser = ArgumentParser()
    parser.add_argument('--dataset', default="bugsinpy", type=str)
    parser.add_argument('--model_inference_dirs', default="patch", type=str)
    parser.add_argument('--k_list', type=str, default="1,2,3,4,5,6,7,8,9,10",
                        help="Comma-separated list of k values, e.g., '1,3,5,10'")
    return parser


def main():
    args = get_parser().parse_args()
    dataset_name = args.dataset
    k_list = [int(k) for k in args.k_list.split(',')]

    for model_inference_dir in args.model_inference_dirs.split(','):
        print(f'Evaluating pass_at_k result for {dataset_name} on {model_inference_dir}')

        _evaluate_path = f"{CURRENT_DIR_PATH}/{dataset_name}/{model_inference_dir}"
        if not os.path.exists(_evaluate_path):
            raise ValueError(f'error: _evaluate_path {_evaluate_path} not exist!!')

        file_pattern = f"{_evaluate_path}/unittest_result_5.PatchesResults10.json"
        eval_files = glob.glob(file_pattern)

        if not eval_files:
            print(f"Warning: No evaluation files found matching pattern: {file_pattern}\n")
            continue

        print(f"Found {len(eval_files)} files to process: {eval_files}")

        combined_eval_results = {}
        for file_path in eval_files:
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    combined_eval_results.update(data)
            except (json.JSONDecodeError, FileNotFoundError) as e:
                print(f"Warning: Could not read or parse {file_path}. Skipping. Error: {e}")

        # <--- ìˆ˜ì •ëœ ë¶€ë¶„: total_bug_count=51 ì¸ìë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
        result_ = _evaluate_average_pass_at_k(combined_eval_results, k_list, total_bug_count=51)

        if result_ is None:
            print(f'Could not calculate pass@k for {model_inference_dir}, skipping.\n')
            continue

        print(f'Result: {result_}\n')

        pass_k_result_path = f"{_evaluate_path}/pass_k_result_5.PatchesResults10_.txt"
        with open(pass_k_result_path, 'a') as pass_k_result:
            pass_k_result.write(f'pass_at_k_final: {result_}\n')


def _evaluate_average_pass_at_k(eval_result_json, k_list, total_bug_count=None):
    """
    total_bug_count: ì „ì²´ ë²¤ì¹˜ë§ˆí¬ì˜ ë²„ê·¸ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
                     Noneì´ë©´ JSONì— ìˆëŠ” ë²„ê·¸ ìˆ˜ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. (ê¸°ì¡´ ë°©ì‹)
    """
    if not eval_result_json:
        print("Error: The provided evaluation data is empty.")
        return None

    pass_at_k_result = {}

    scores_per_bug = []
    for bug_id, eval_result in eval_result_json.items():
        if 'nucleus_sampling_flags' not in eval_result:
            continue

        flags = eval_result['nucleus_sampling_flags']
        n = len(flags)
        c = flags.count('Pass')

        if n == 0:
            continue

        scores_per_bug.append({'n': n, 'c': c})

    if not scores_per_bug:
        print("Warning: No valid bug results found to calculate pass@k.")
        return {}

    # --- ë¡œì§ ìˆ˜ì • ë¶€ë¶„ ì‹œì‘ ---

    # í‰ê°€ì˜ ê¸°ì¤€ì´ ë  ì „ì²´ ë²„ê·¸ ìˆ˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    # ì¸ìë¡œ ë°›ì€ total_bug_countê°€ ìˆìœ¼ë©´ ê·¸ ê°’ì„ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ì²˜ë¦¬ëœ ë²„ê·¸ ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    processed_bugs_count = len(scores_per_bug)
    bugs_to_evaluate_on = total_bug_count if total_bug_count is not None else processed_bugs_count

    # ì„±ê³µí•œ ë²„ê·¸ ìˆ˜ëŠ” 'c'ê°€ 0ë³´ë‹¤ í° ë²„ê·¸ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. ì´ ë¡œì§ì€ ë™ì¼í•©ë‹ˆë‹¤.
    solved_bugs = sum(1 for item in scores_per_bug if item['c'] > 0)

    for k in sorted(k_list):
        # ì²˜ë¦¬ëœ ë²„ê·¸ë“¤ì˜ pass@k ì ìˆ˜ì˜ 'ì´í•©'ì„ ë¨¼ì € ê³„ì‚°í•©ë‹ˆë‹¤.
        # ê²°ê³¼ê°€ ì—†ëŠ” ë²„ê·¸ëŠ” ì ìˆ˜ê°€ 0ì´ë¯€ë¡œ í•©ì‚°ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.
        total_pass_at_k_score = sum(_compute_pass_at_k(item['n'], item['c'], k) for item in scores_per_bug)

        # 'ì´í•©'ì„ ì „ì²´ ê¸°ì¤€ ë²„ê·¸ ìˆ˜(ì˜ˆ: 51)ë¡œ ë‚˜ëˆ„ì–´ ì˜¬ë°”ë¥¸ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        pass_at_k = total_pass_at_k_score / bugs_to_evaluate_on

        # ì¶œë ¥ í˜•ì‹ì—ì„œ ë¶„ëª¨ë¥¼ ì˜¬ë°”ë¥¸ ê¸°ì¤€ ë²„ê·¸ ìˆ˜ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
        pass_at_k_result[f"pass@{k}"] = f"{pass_at_k:.2%} ({solved_bugs}/{bugs_to_evaluate_on})"

    # --- ë¡œì§ ìˆ˜ì • ë¶€ë¶„ ë ---

    return pass_at_k_result


def _compute_pass_at_k(n, c, k):
    """
    pass@kë¥¼ í‘œì¤€ ì¡°í•©(combination) ê³µì‹ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê²Œ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    n: ì „ì²´ ìƒ˜í”Œ ìˆ˜
    c: ì„±ê³µ(í†µê³¼) ìƒ˜í”Œ ìˆ˜
    k: ì„ íƒí•  ìƒ˜í”Œ ìˆ˜
    """
    # ğŸ’¡ [ìˆ˜ì • 1] ì„±ê³µ ìƒ˜í”Œì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´(c=0), í™•ë¥ ì€ í•­ìƒ 0ì…ë‹ˆë‹¤.
    if c == 0:
        return 0.0

    # ğŸ’¡ [ìˆ˜ì • 2] ì‹¤íŒ¨ ìƒ˜í”Œ ìˆ˜(n-c)ê°€ ë½‘ìœ¼ë ¤ëŠ” ìˆ˜(k)ë³´ë‹¤ ì ìœ¼ë©´, ë°˜ë“œì‹œ ì„±ê³µ ìƒ˜í”Œì„ í¬í•¨í•˜ê²Œ ë˜ë¯€ë¡œ í™•ë¥ ì€ 1ì…ë‹ˆë‹¤.
    if n - c < k:
        return 1.0

    # ğŸ’¡ [ìˆ˜ì • 3] í‘œì¤€ ì¡°í•© ê³µì‹ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    # pass@k = 1 - (kê°œì˜ ì‹¤íŒ¨ ìƒ˜í”Œë§Œ ë½‘ì„ í™•ë¥ )
    return 1.0 - (comb(n - c, k) / comb(n, k))


if __name__ == "__main__":
    main()